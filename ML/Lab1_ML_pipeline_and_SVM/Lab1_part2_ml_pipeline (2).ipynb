{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment â„–1, part 2\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*This is the second part of the assignment. First and third parts are waiting for you in the same directory.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-512ba712fc0fc065",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Part 2. Data preprocessing, model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b656a4266174b009",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 1. Reading the data\n",
    "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on colab, uncomment the following lines\n",
    "\n",
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/basic_s21/homeworks_basic/Lab1_ML_pipeline_and_SVM/car_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eebac6bfdf73d0bc",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(846, 19) (846,)\n",
      "(549, 19) (549,) (297, 19) (297,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(333)\n",
    "\n",
    "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
    "data = dataset[:, :-1].astype(int)\n",
    "target = dataset[:, -1]\n",
    "\n",
    "print(data.shape, target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88b1a0f688568f2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397</td>\n",
       "      <td>89</td>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>162</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>153</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>136</td>\n",
       "      <td>175</td>\n",
       "      <td>352</td>\n",
       "      <td>154</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>390</td>\n",
       "      <td>86</td>\n",
       "      <td>42</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>152</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>141</td>\n",
       "      <td>169</td>\n",
       "      <td>332</td>\n",
       "      <td>171</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>179</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>545</td>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>84</td>\n",
       "      <td>135</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>155</td>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>158</td>\n",
       "      <td>176</td>\n",
       "      <td>351</td>\n",
       "      <td>164</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>183</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>537</td>\n",
       "      <td>86</td>\n",
       "      <td>40</td>\n",
       "      <td>66</td>\n",
       "      <td>139</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>139</td>\n",
       "      <td>145</td>\n",
       "      <td>225</td>\n",
       "      <td>143</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>202</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352</td>\n",
       "      <td>91</td>\n",
       "      <td>46</td>\n",
       "      <td>78</td>\n",
       "      <td>148</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>147</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>152</td>\n",
       "      <td>168</td>\n",
       "      <td>323</td>\n",
       "      <td>199</td>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>189</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>620</td>\n",
       "      <td>108</td>\n",
       "      <td>56</td>\n",
       "      <td>103</td>\n",
       "      <td>234</td>\n",
       "      <td>73</td>\n",
       "      <td>10</td>\n",
       "      <td>221</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>232</td>\n",
       "      <td>718</td>\n",
       "      <td>214</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>492</td>\n",
       "      <td>85</td>\n",
       "      <td>43</td>\n",
       "      <td>66</td>\n",
       "      <td>130</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>151</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>143</td>\n",
       "      <td>173</td>\n",
       "      <td>333</td>\n",
       "      <td>168</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>180</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>231</td>\n",
       "      <td>85</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>115</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>61</td>\n",
       "      <td>17</td>\n",
       "      <td>119</td>\n",
       "      <td>130</td>\n",
       "      <td>184</td>\n",
       "      <td>127</td>\n",
       "      <td>86</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>181</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>79</td>\n",
       "      <td>89</td>\n",
       "      <td>44</td>\n",
       "      <td>68</td>\n",
       "      <td>113</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>147</td>\n",
       "      <td>171</td>\n",
       "      <td>328</td>\n",
       "      <td>189</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>179</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>411</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "      <td>51</td>\n",
       "      <td>116</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>57</td>\n",
       "      <td>17</td>\n",
       "      <td>124</td>\n",
       "      <td>135</td>\n",
       "      <td>209</td>\n",
       "      <td>135</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>177</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>825</td>\n",
       "      <td>86</td>\n",
       "      <td>39</td>\n",
       "      <td>84</td>\n",
       "      <td>149</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>156</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>185</td>\n",
       "      <td>358</td>\n",
       "      <td>157</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>183</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>364</td>\n",
       "      <td>87</td>\n",
       "      <td>45</td>\n",
       "      <td>77</td>\n",
       "      <td>153</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>154</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>145</td>\n",
       "      <td>181</td>\n",
       "      <td>350</td>\n",
       "      <td>172</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>184</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>758</td>\n",
       "      <td>81</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>152</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>151</td>\n",
       "      <td>167</td>\n",
       "      <td>333</td>\n",
       "      <td>179</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>179</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>506</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>112</td>\n",
       "      <td>201</td>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>214</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>162</td>\n",
       "      <td>223</td>\n",
       "      <td>667</td>\n",
       "      <td>194</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>190</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>550</td>\n",
       "      <td>86</td>\n",
       "      <td>38</td>\n",
       "      <td>58</td>\n",
       "      <td>119</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>118</td>\n",
       "      <td>57</td>\n",
       "      <td>17</td>\n",
       "      <td>129</td>\n",
       "      <td>140</td>\n",
       "      <td>208</td>\n",
       "      <td>152</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1   2    3    4   5   6    7   8   9    10   11   12   13  14  15  \\\n",
       "0   397   89  41   75  162  66   5  153  43  19  136  175  352  154  72   2   \n",
       "1   390   86  42   65  113  50   8  152  45  19  141  169  332  171  85   4   \n",
       "2   545   88  44   84  135  55  12  155  44  20  158  176  351  164  75   7   \n",
       "3   537   86  40   66  139  59   7  122  54  17  139  145  225  143  63   7   \n",
       "4   352   91  46   78  148  61   9  147  45  19  152  168  323  199  70  13   \n",
       "5   620  108  56  103  234  73  10  221  30  25  174  232  718  214  73   8   \n",
       "6   492   85  43   66  130  57   6  151  45  19  143  173  333  168  86   4   \n",
       "7   231   85  33   40  115  57   3  112  61  17  119  130  184  127  86  12   \n",
       "8    79   89  44   68  113  50   7  150  45  19  147  171  328  189  88   6   \n",
       "9   411   78  36   51  116  56   4  120  57  17  124  135  209  135  84   1   \n",
       "10  825   86  39   84  149  57   8  156  43  20  133  185  358  157  74   0   \n",
       "11  364   87  45   77  153  59   7  154  44  19  145  181  350  172  75  15   \n",
       "12  758   81  45   68  137  60   7  152  45  19  151  167  333  179  81   3   \n",
       "13  506  101  51  112  201  59  11  214  32  24  162  223  667  194  65   0   \n",
       "14  550   86  38   58  119  56   4  118  57  17  129  140  208  152  78   9   \n",
       "\n",
       "    16   17   18  \n",
       "0    0  188  195  \n",
       "1   16  179  183  \n",
       "2   11  183  195  \n",
       "3   11  202  208  \n",
       "4   11  189  200  \n",
       "5    3  187  197  \n",
       "6    9  180  183  \n",
       "7   21  181  183  \n",
       "8    5  179  182  \n",
       "9   12  177  184  \n",
       "10  23  183  190  \n",
       "11  14  184  189  \n",
       "12  13  179  183  \n",
       "13  36  190  206  \n",
       "14   2  184  186  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd = pd.DataFrame(X_train)\n",
    "\n",
    "# First 15 rows of our dataset.\n",
    "X_train_pd.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98e7d91d77d65fcf",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Methods `describe` and `info` deliver some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>418.712204</td>\n",
       "      <td>93.728597</td>\n",
       "      <td>44.690346</td>\n",
       "      <td>81.983607</td>\n",
       "      <td>168.511840</td>\n",
       "      <td>61.502732</td>\n",
       "      <td>8.530055</td>\n",
       "      <td>168.601093</td>\n",
       "      <td>41.052823</td>\n",
       "      <td>20.584699</td>\n",
       "      <td>147.673953</td>\n",
       "      <td>188.333333</td>\n",
       "      <td>439.295082</td>\n",
       "      <td>173.559199</td>\n",
       "      <td>72.307832</td>\n",
       "      <td>6.453552</td>\n",
       "      <td>12.493625</td>\n",
       "      <td>189.034608</td>\n",
       "      <td>195.712204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>240.487008</td>\n",
       "      <td>8.360989</td>\n",
       "      <td>6.218256</td>\n",
       "      <td>15.764974</td>\n",
       "      <td>33.402139</td>\n",
       "      <td>7.919905</td>\n",
       "      <td>4.469024</td>\n",
       "      <td>33.929286</td>\n",
       "      <td>7.963588</td>\n",
       "      <td>2.638607</td>\n",
       "      <td>14.687787</td>\n",
       "      <td>32.312683</td>\n",
       "      <td>180.461696</td>\n",
       "      <td>32.811598</td>\n",
       "      <td>7.566611</td>\n",
       "      <td>4.933881</td>\n",
       "      <td>8.820310</td>\n",
       "      <td>6.191639</td>\n",
       "      <td>7.459933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>422.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>618.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>586.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>845.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>211.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean   418.712204   93.728597   44.690346   81.983607  168.511840   61.502732   \n",
       "std    240.487008    8.360989    6.218256   15.764974   33.402139    7.919905   \n",
       "min      0.000000   73.000000   33.000000   40.000000  104.000000   47.000000   \n",
       "25%    214.000000   87.000000   40.000000   70.000000  141.000000   57.000000   \n",
       "50%    422.000000   93.000000   44.000000   80.000000  167.000000   61.000000   \n",
       "75%    618.000000  100.000000   49.000000   98.000000  194.000000   65.000000   \n",
       "max    845.000000  119.000000   59.000000  112.000000  333.000000  138.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean     8.530055  168.601093   41.052823   20.584699  147.673953  188.333333   \n",
       "std      4.469024   33.929286    7.963588    2.638607   14.687787   32.312683   \n",
       "min      3.000000  112.000000   26.000000   17.000000  118.000000  130.000000   \n",
       "25%      6.000000  145.000000   33.000000   19.000000  136.000000  166.000000   \n",
       "50%      8.000000  157.000000   43.000000   20.000000  146.000000  178.000000   \n",
       "75%     10.000000  198.000000   46.000000   23.000000  159.000000  217.000000   \n",
       "max     52.000000  265.000000   61.000000   29.000000  188.000000  320.000000   \n",
       "\n",
       "                12          13          14          15          16  \\\n",
       "count   549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean    439.295082  173.559199   72.307832    6.453552   12.493625   \n",
       "std     180.461696   32.811598    7.566611    4.933881    8.820310   \n",
       "min     184.000000  109.000000   60.000000    0.000000    0.000000   \n",
       "25%     313.000000  147.000000   67.000000    2.000000    5.000000   \n",
       "50%     363.000000  172.000000   71.000000    6.000000   11.000000   \n",
       "75%     586.000000  198.000000   75.000000   10.000000   18.000000   \n",
       "max    1018.000000  268.000000  135.000000   22.000000   41.000000   \n",
       "\n",
       "               17          18  \n",
       "count  549.000000  549.000000  \n",
       "mean   189.034608  195.712204  \n",
       "std      6.191639    7.459933  \n",
       "min    176.000000  181.000000  \n",
       "25%    184.000000  191.000000  \n",
       "50%    189.000000  197.000000  \n",
       "75%    193.000000  201.000000  \n",
       "max    206.000000  211.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       549 non-null    int64\n",
      " 1   1       549 non-null    int64\n",
      " 2   2       549 non-null    int64\n",
      " 3   3       549 non-null    int64\n",
      " 4   4       549 non-null    int64\n",
      " 5   5       549 non-null    int64\n",
      " 6   6       549 non-null    int64\n",
      " 7   7       549 non-null    int64\n",
      " 8   8       549 non-null    int64\n",
      " 9   9       549 non-null    int64\n",
      " 10  10      549 non-null    int64\n",
      " 11  11      549 non-null    int64\n",
      " 12  12      549 non-null    int64\n",
      " 13  13      549 non-null    int64\n",
      " 14  14      549 non-null    int64\n",
      " 15  15      549 non-null    int64\n",
      " 16  16      549 non-null    int64\n",
      " 17  17      549 non-null    int64\n",
      " 18  18      549 non-null    int64\n",
      "dtypes: int64(19)\n",
      "memory usage: 81.6 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be844269be69c387",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 2. Machine Learning pipeline\n",
    "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Data preprocessing\n",
    "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many elements of each class in train dataset. Dataset shouldn't be unbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of class bus: 133\n",
      "Count of class opel: 141\n",
      "Count of class saab: 142\n",
      "Count of class van: 133\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "for class_name in np.unique(y_train):\n",
    "    print(f\"Count of class {class_name}: {(y_train == class_name).sum()}\")\n",
    "\n",
    "min_num_each = max((y_train == class_name).sum() for class_name in \n",
    "                   np.unique(y_train))\n",
    "print(min_num_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(X: np.ndarray, y: np.ndarray, min_num_each: int):\n",
    "    classes = np.unique(y)\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    for class_name in classes:\n",
    "        count = (y == class_name).sum()\n",
    "        need_add = max(0, min_num_each - count)\n",
    "        X_cur_class = X[y == class_name,:]\n",
    "        X_new.append(X_cur_class)\n",
    "        X_new.append(X_cur_class[np.random.randint(0, count, size=need_add)])\n",
    "        y_new.append(np.array([class_name] * (count + need_add)))\n",
    "    X_new = np.concatenate(X_new)\n",
    "    y_new = np.concatenate(y_new)\n",
    "    indeces = np.arange(len(X_new))\n",
    "    np.random.shuffle(indeces)\n",
    "    return X_new[indeces], y_new[indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of class bus: 142\n",
      "Count of class opel: 142\n",
      "Count of class saab: 142\n",
      "Count of class van: 142\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = balance_classes(X_train, y_train, min_num_each)\n",
    "\n",
    "for class_name in np.unique(y_train):\n",
    "    print(f\"Count of class {class_name}: {(y_train == class_name).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do feature scaling. We need features to have the equal mean and scale to make PCA and linear models with regularization work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a1514aa189a49fca",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def make_basic_pipeline(model):\n",
    "    return Pipeline([\n",
    "        (\"Scaler\", StandardScaler()),\n",
    "        (\"Model\", model),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Basic logistic regression\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "def find_best_params(model, params_grid, X_train, y_train, X_test, y_test):\n",
    "    searcher = GridSearchCV(\n",
    "        model,\n",
    "        params_grid,\n",
    "        scoring=\"accuracy\",\n",
    "        refit=True,\n",
    "    )\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        searcher.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters:\", searcher.best_params_)\n",
    "    model = searcher.best_estimator_\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1 score:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    return model, searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1dd5ad5d0845cbbb",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'Model__C': 15, 'Model__penalty': 'l2'}\n",
      "Accuracy score: 0.8114478114478114\n",
      "F1 score: 0.8075045050555744\n"
     ]
    }
   ],
   "source": [
    "params_grid = {\n",
    "    \"Model__C\": [0.1, 1, 2, 5, 10, 15, 25, 50, 75, 100, 200],\n",
    "    \"Model__penalty\": [\"l1\", \"l2\"],\n",
    "}\n",
    "\n",
    "model = make_basic_pipeline(\n",
    "    LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"saga\",\n",
    "        tol=1e-3,\n",
    "        max_iter=500,\n",
    "    )\n",
    ")\n",
    "\n",
    "model, best_params = find_best_params(model, params_grid, X_train, y_train,\n",
    "                                      X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scikitplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3333844e283f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ! pip install scikit-plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikitplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_roc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikitplot'"
     ]
    }
   ],
   "source": [
    "# You might use this command to install scikit-plot. \n",
    "# Warning, if you a running locally, don't call pip from within jupyter, call it from terminal in the corresponding \n",
    "# virtual environment instead\n",
    "\n",
    "# ! pip install scikit-plot\n",
    "\n",
    "from scikitplot.metrics import plot_roc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "plot_roc(y_test, y_pred_proba, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. PCA: explained variance plot\n",
    "* Apply the PCA to the train part of the data. Build the explaided variance plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c6c614740bce090e",
     "locked": false,
     "points": 10,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "pca = PCA()\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "explained_var_cumsum = np.cumsum(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Explained variance\")\n",
    "plt.bar(range(1, num_features + 1), pca.explained_variance_ratio_, alpha=0.3, \n",
    "        align=\"center\", label=\"Explained variance\")\n",
    "plt.step(range(1, num_features + 1), explained_var_cumsum,\n",
    "         label=\"Cumulative explained variance\", linewidth=2, where=\"mid\")\n",
    "plt.xticks(range(1, num_features + 1))\n",
    "plt.yticks(np.arange(0, 1.01, 0.05))\n",
    "plt.xlabel(\"Principal component\")\n",
    "plt.ylabel(\"Explained variance ratio\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually number of components is chosen to make explained variance be ~95%. So I will do it. Here it is 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = Pipeline([\n",
    "    (\"Scale\", StandardScaler()),\n",
    "    (\"PCA\", PCA(n_components=0.95)),\n",
    "]).fit_transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Dataset after PCA\")\n",
    "for class_name in np.unique(y_train):\n",
    "    indeces = (y_train == class_name)\n",
    "    plt.scatter(X_pca[indeces,0], X_pca[indeces,1], label=class_name)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transformation classes seem to be mixed. We can only clearly see red cluster on left side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c1fe666f52fe53c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.3. PCA trasformation\n",
    "* Select the appropriate number of components. Briefly explain your choice. Should you normalize the data?\n",
    "\n",
    "*Use `fit` and `transform` methods to transform the `train` and `test` parts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-96ab18d96473ef71",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_pca_pipeline(model):\n",
    "    return Pipeline([\n",
    "        (\"Scale\", StandardScaler()),\n",
    "        (\"PCA\", PCA(n_components=0.95)),\n",
    "        (\"Scale after PCA\", StandardScaler()),\n",
    "        (\"Model\", model),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: From this point `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d28b58a35c94e988",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.4. Logistic regression on PCA-preprocessed data.\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the transformed by PCA `train` data.\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` and `tol=1e-3`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-12d53ea45258fa82",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"Model__C\": [0.1, 1, 2, 5, 10, 15, 25, 50, 75, 100, 200],\n",
    "    \"Model__penalty\": [\"l1\", \"l2\"],\n",
    "}\n",
    "\n",
    "model = make_pca_pipeline(\n",
    "    LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"saga\",\n",
    "        tol=1e-3,\n",
    "    )\n",
    ")\n",
    "\n",
    "model, log_reg_best_params = find_best_params(model, params_grid, X_train,\n",
    "                                              y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)\n",
    "plot_roc(y_test, y_pred_proba, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fbf16c64076e139",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.5. Decision tree\n",
    "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
    "\n",
    "* Measure the model quality using the same metrics you used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-748ed20b51c67fab",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params_grid = {\n",
    "    \"Model__max_depth\": [3, 5, 7, 10, 15],\n",
    "}\n",
    "\n",
    "model = make_pca_pipeline(DecisionTreeClassifier())\n",
    "\n",
    "model, tree_best_params = find_best_params(model, params_grid, X_train,\n",
    "                                           y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9eadd4d8a03ae67a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "#### 2.6. Bagging.\n",
    "Here starts the ensembling part.\n",
    "\n",
    "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
    "\n",
    "We will build two ensembles: of logistic regressions and of decision trees.\n",
    "\n",
    "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
    "\n",
    "\n",
    "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
    "\n",
    "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
    "\n",
    "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
    "\n",
    "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
    "\n",
    "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "def test_bagging(n_estimators: int,\n",
    "                 base_estimator,\n",
    "                 X_train,\n",
    "                 y_train,\n",
    "                 X_test,\n",
    "                 y_test):\n",
    "    bagging = make_pca_pipeline(\n",
    "        BaggingClassifier(base_estimator, n_estimators))\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        bagging.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = bagging.predict(X_test)\n",
    "    return f1_score(y_test, y_pred, average=\"macro\"),\\\n",
    "           accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list = [*range(2, 100, 5), 100]\n",
    "n_estimators_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"saga\",\n",
    "    tol=1e-3,\n",
    "    **{key.split(\"__\")[1]: val for key, val in log_reg_best_params.items()}\n",
    ")\n",
    "\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    f1, acc = test_bagging(n_estimators, base_estimator, X_train, y_train,\n",
    "                           X_test, y_test)\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Score of bagging of logistic regressions \"\n",
    "          \"for different ensemble size\")\n",
    "plt.plot(n_estimators_list, f1_scores, label=\"F1 score\", linewidth=2.5)\n",
    "plt.plot(n_estimators_list, acc_scores, label=\"Accuracy\", linewidth=2.5)\n",
    "plt.xlabel(\"n estimators\")\n",
    "plt.xticks(n_estimators_list)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of logistic regressions doesn't improve quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = DecisionTreeClassifier(\n",
    "    **{key.split(\"__\")[1]: val for key, val in tree_best_params.items()}\n",
    ")\n",
    "\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    f1, acc = test_bagging(n_estimators, base_estimator, X_train, y_train,\n",
    "                           X_test, y_test)\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Score of bagging of decision trees for different ensemble size\")\n",
    "plt.plot(n_estimators_list, f1_scores, label=\"F1 score\", linewidth=2.5)\n",
    "plt.plot(n_estimators_list, acc_scores, label=\"Accuracy\", linewidth=2.5)\n",
    "plt.xlabel(\"n estimators\")\n",
    "plt.xticks(n_estimators_list)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal number of trees ~35. Further there is not significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-241b7691ab44cbfb",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.7. Random Forest\n",
    "Now we will work with the Random Forest (its `sklearn` implementation).\n",
    "\n",
    "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
    "\n",
    "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-888755d0f3d91620",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    model = make_pca_pipeline(\n",
    "        RandomForestClassifier(n_estimators=n_estimators,\n",
    "                               max_depth=tree_best_params[\"Model__max_depth\"])\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Score of random forest for different ensemble size\")\n",
    "plt.plot(n_estimators_list, f1_scores, label=\"F1 score\", linewidth=2.5)\n",
    "plt.plot(n_estimators_list, acc_scores, label=\"Accuracy\", linewidth=2.5)\n",
    "plt.xlabel(\"n estimators\")\n",
    "plt.xticks(n_estimators_list)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparision with naive bagging of trees random forest needs less ensemble size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-99191c0852538d4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.8. Learning curve\n",
    "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
    "\n",
    "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
    "\n",
    "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
    "\n",
    "* Analyse the final plot. Can you make any conlusions using it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e39bc7e7dff61ff9",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def split_into_parts(size: int, n_parts: int):\n",
    "    part_size = size // n_parts\n",
    "    last_index = 0\n",
    "    indeces = list(range(size))\n",
    "    for i in range(n_parts):\n",
    "        if (size - i * part_size) // (part_size + 1) == n_parts - i:\n",
    "            part_size += 1\n",
    "        last_index += part_size\n",
    "        yield indeces[:last_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(split_into_parts(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parts = 10\n",
    "x_ticks = list(range(1, n_parts + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"saga\",\n",
    "    tol=1e-3,\n",
    "    **{key.split(\"__\")[1]: val for key, val in log_reg_best_params.items()}\n",
    ")\n",
    "\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "for train_indeces in split_into_parts(len(X_train), n_parts):\n",
    "    model = make_pca_pipeline(base_estimator)\n",
    "    model.fit(X_train[train_indeces], y_train[train_indeces])\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Score of logistic regression with different train size\")\n",
    "plt.plot(x_ticks, f1_scores, label=\"F1 score\", linewidth=2.5)\n",
    "plt.plot(x_ticks, acc_scores, label=\"Accuracy\", linewidth=2.5)\n",
    "plt.xlabel(\"Train parts\")\n",
    "plt.xticks(ticks=x_ticks,\n",
    "           labels=[\"$\\\\frac{%d}{%d}$\" % (p, n_parts) for p in x_ticks])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = DecisionTreeClassifier(\n",
    "    **{key.split(\"__\")[1]: val for key, val in tree_best_params.items()}\n",
    ")\n",
    "\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "for train_indeces in split_into_parts(len(X_train), n_parts):\n",
    "    model = make_pca_pipeline(base_estimator)\n",
    "    model.fit(X_train[train_indeces], y_train[train_indeces])\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Score of decision tree with different train size\")\n",
    "plt.plot(x_ticks, f1_scores, label=\"F1 score\", linewidth=2.5)\n",
    "plt.plot(x_ticks, acc_scores, label=\"Accuracy\", linewidth=2.5)\n",
    "plt.xlabel(\"Train parts\")\n",
    "plt.xticks(ticks=x_ticks,\n",
    "           labels=[\"$\\\\frac{%d}{%d}$\" % (p, n_parts) for p in x_ticks])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = RandomForestClassifier(\n",
    "    n_estimators=17,\n",
    "    max_depth=tree_best_params[\"Model__max_depth\"],\n",
    ")\n",
    "\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "for train_indeces in split_into_parts(len(X_train), n_parts):\n",
    "    model = make_pca_pipeline(base_estimator)\n",
    "    model.fit(X_train[train_indeces], y_train[train_indeces])\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "plt.title(\"Score of random forest with different train size\")\n",
    "plt.plot(x_ticks, f1_scores, label=\"F1 score\", linewidth=2.5)\n",
    "plt.plot(x_ticks, acc_scores, label=\"Accuracy\", linewidth=2.5)\n",
    "plt.xlabel(\"Train parts\")\n",
    "plt.xticks(ticks=x_ticks,\n",
    "           labels=[\"$\\\\frac{%d}{%d}$\" % (p, n_parts) for p in x_ticks])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all models bigger dataset size = better. But on linear regression from $\\frac{5}{10}$ of size there is plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
